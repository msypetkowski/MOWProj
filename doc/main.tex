\documentclass[a4paper]{article}

\usepackage[a4paper,  margin=0.5in]{geometry}

\usepackage{graphicx}
\usepackage{float}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{longtable}



\usepackage[utf8]{inputenc}
\begin{document}


\title{Prediction of student's alcohol consumption with random forests}

\author{Mikołaj Ciesielski, Michał Sypetkowski}
\setlength\columnsep{0.375in}  \newlength\titlebox \setlength\titlebox{2.25in}
\twocolumn
\maketitle

% \tableofcontents
% \newpage

% \begin{multicols}{2}

\section{Data}

Research is done with dataset: \url{https://www.kaggle.com/uciml/student-alcohol-consumption/}.

The data were obtained in a survey of students
math and portuguese language courses in secondary school.
It contains a lot of interesting social,
gender and study information about students.

There are several (382) students that belong to both datasets.
These students can be identified by searching for identical attributes
that characterize each student, as shown in the annexed R file.

The dataset contains in total 395 math-course-students samples and 
649 portugese-language-studens samples. Each sample has 33 atributes.

For experiments, we merged both tables removing duplicate students.
We replace students grades attributes
(\texttt{G1}, \texttt{G2} and \texttt{G3})
with
\texttt{M\_G1},
\texttt{M\_G2},
\texttt{M\_G3},
\texttt{P\_G1},
\texttt{P\_G2} and
\texttt{P\_G3}.
Attribute \texttt{paid} also concerns particular course, so we split it into
\texttt{M\_paid} and \texttt{P\_paid}.
Since randomForest package (see section \ref{forest}) doesn't support surrogate splits,
missing values are replaced with mean in case of grades and by mode in case of paid attribute.
Final dataset consists of 662 examples (37 attributes).

\section{Clustering Dalc and Walc attributes into one binary attribute}
\label{clust}

We aim to build a model that would perform binary classification --
whether student can be considered drinking alcohol regularly or not.
The dataset provides 2 attributes:
\begin{itemize}
    \item Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)
    \item Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)
\end{itemize}
2D histogram visualization is shown on figure \ref{fig:hist2D}.

First, we standarize Dalc and Walc attributes.
Using k-means algorithm (k=2) with initial centers in (-1,-1) and (1,1), we got clustering as shown in figure \ref{fig:clust}.
In the end, we classify 45.3\% of the students as drinking with proposed clustering.
The results may be considered commonsensial.


% TODO: think about following
% We maximize consistency within 2 clusters of data using Silhouette coeficient
% (\url{https://en.wikipedia.org/wiki/Silhouette_(clustering)}).
% We use L2 distance.

\begin{figure}[!hbt]
    \centering
    \includegraphics[page=1,width=0.5\textwidth]{../Rplots.pdf}
    \label{fig:hist2D}
    \caption[]{2D histogram of Dalc and Walc attributes}
\end{figure}

\begin{figure}[!hbt]
    \centering
    \includegraphics[page=2,width=0.5\textwidth]{../Rplots.pdf}
    \label{fig:clust}
    \caption[]{Clustering with k-means algorithm. (1: non-drinking, 2: drinking)}
\end{figure}

\section{Single attributes}
\label{xent}
First, we check how the class is depenent on each single nominal attribute.

Figure \ref{fig:nominalIG} shows information gain values
of single splits by each nominal attribute (in the context of class Drink described in section \ref{clust}).
As we can see, sex of the students gives significantly bigger information gain
than the other nominal attributes.

% Table \ref{table:nominalIG} shows information gain values for all nominal attributes.
% \input{../nominalIG}

\begin{figure}[!hbt]
    \centering
    \includegraphics[page=5,width=0.5\textwidth]{../Rplots.pdf}
    \caption[]{Information gain values for all nominal attributes
    \label{fig:nominalIG}
    }
\end{figure}

\begin{figure}[!hbt]
    \centering
    \includegraphics[page=6,width=0.5\textwidth]{../Rplots.pdf}
    \caption[]{P-values for all numeric attributes
    \label{fig:pval}
    }
\end{figure}


Figure \ref{fig:pval} shows p-values of t-test significance for numeric attributes.
From this plot, we can decide which attributes are statistically important
in the context of our classification problem.



\section{Experiments with single decision trees}
\label{expSingle}

Accuracy of decision tree shouldn't give better results than random forest,
so we trained several single decision trees to establish
an upper limit of desired error rate for random forests.

We used rpart\footnote{\url{https://cran.r-project.org/web/packages/rpart/rpart.pdf}} package for the experiments.
We trained many models (with different parameters).
We experimented with:
\begin{enumerate}
    \item cp - complexity parameter.
        Any split that does not decrease the overall lack of fit by a factor of cp is not attempted
        (the lower cp is, the more complex the model is)
    \item minbucket - the minimum number of observations in any terminal node
    \item maxdepth - maximum tree depth
    \item split - method for calculating how good is a division (Gini index or enthropy)
\end{enumerate}
In accuracy measuring, we use cross-validation with 8 partitions
and 10 repetitions (random shuffles of dataset before partitioning).
% Figure \ref{fig:single1} shows best tree for math studens,
% figure \ref{fig:single2} for portuguese-language studens.
% 
\subsection{Results}
\label{singleConc}

Results for all trained decision trees are shown in table \ref{table:singleResults}.
Trees with lowest error are simple and we can see that 2 attributes are most important:
\begin{itemize}
    \item goout - going out with friends (numeric: from 1 - very low to 5 - very high).
        Splits with this attribute are done by comparing value of this attribute to 3.5
        (less or equal 3 or higher than 3).
    \item sex - student's sex (binary: 'F' - female or 'M' - male).
        Trees use generalization, such that female students are less likely to drink alcochol.
\end{itemize}
Such tree is shown in figure \ref{fig:single}.

\begin{figure}[!hbt]
    \centering
    % \includegraphics[trim={2cm 2cm 2cm 2cm},clip,page=3,width=0.5\textwidth]{../Rplots.pdf}
    \includegraphics[trim={1cm 1cm 1cm 1cm},clip,page=3,width=0.5\textwidth]{../Rplots.pdf}
    \label{fig:single}
    \caption[]{Single decision tree with lowest mean error}
\end{figure}

% \begin{figure}[]
%     \caption[]{Best single decision tree for math students that we achieved}
%     \centering
%     \includegraphics[page=4,width=1.0\textwidth]{../Rplots.pdf}
%     \label{fig:single1}
% \end{figure}
% 
% \begin{figure}[]
%     \caption[]{Best single decision tree for portugese students that we achieved}
%     \centering
%     \includegraphics[page=6,width=1.0\textwidth]{../Rplots.pdf}
%     \label{fig:single2}
% \end{figure}
% 
% 
% \newpage
\section{Experiments with random forests}
\label{forest}
We used randomForest\footnote{\url{https://cran.r-project.org/web/packages/randomForest/randomForest.pdf}} package for the experiments.
We trained many models (with different parameters).
In accuracy measuring, we use cross-validation with 8 partitions
and 10 repetitions (similarly as in \ref{expSingle}).
We experimented with:
\begin{enumerate}
    \item ntree - number of trees in forest
    \item nodesize - minimum size (number of assiociated examples) of terminal nodes.
    % \item seed - seed used when building a tree
    %         In case of small random forests, it may be important to experiment with different random seed.
    %         Some implementations may use separate random generator objects for each tree, so that
    %         even with different data (we use cross validation), structure will be similar.
    %         Just in case, we decided to try different seeds set in moment of calling \texttt{randomForest}
    %         function from randomForest package.
    \item mtry - number of variables (attributes) randomly sampled as candidates at each split.
    \item maxnodes - maximum number of terminal nodes that trees in the forest can have.
\end{enumerate}

\subsection{Results}
\label{singleConc}
Results for all tested random forests are shown in table \ref{table:forestResults}.

Random forests doesn't produce better results than single decision trees.
% Moreover, mtry parameter equals total attributes count.
% In results, such random forests have similar properties to single decision trees.
% We found out, that the trees in such forests use mostly splits as
% in case of best single decision trees (see section \ref{singleConc})
Variable importance plot for best random forest parameters (that we have found)
is shown in figure \ref{fig:importance}.


\begin{figure}[!hbt]
    \centering
    \includegraphics[trim={0 0 0 2cm},clip,page=4,width=0.5\textwidth]{../Rplots.pdf}
    \label{fig:importance}
    \caption[]{Variable (attribute) importance plot for random forest with lowest mean error}
\end{figure}


% \begin{figure}[]
%     \caption[]{Variable (attribute) importance plot for math students}
%     \centering
%     \includegraphics[page=5,width=1.0\textwidth]{../Rplots.pdf}
%     \label{fig:imp1}
% \end{figure}
% 
% \begin{figure}[]
%     \caption[]{Variable (attribute) importance plot for portugese language students}
%     \centering
%     \includegraphics[page=7,width=1.0\textwidth]{../Rplots.pdf}
%     \label{fig:imp2}
% \end{figure}
% 
\section{Conclusion}

Random forests doesn't perform well on this dataset (no better than single decision trees).
% Most attributes are noisy in terms of single splits (cross entropy -- see section \ref{xent}).
% Best single decision tree turned out to have only 2 splits (see figure \ref{fig:single}).

% TODO
% Decision trees (and random forests) are trying to use these noisy variables
% by greedily splitting them by some information-gain-like measurement.
% hence they accuracy is similar or lower than accuracy of
% a simple decision tree.


% \end{multicols}

\onecolumn
\newpage
\appendix
\section{All single tree classifiers results}
% \label{singleResults}
% \input{singleResults}
\input{../singleResults}

\newpage
\section{All random forest classifiers results}
% \label{forestResults}
% \input{forestResults}
\input{../forestResults}




\end{document}
