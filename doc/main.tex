\documentclass[a4paper]{article}

\usepackage[a4paper,  margin=0.4in]{geometry}

\usepackage{graphicx}
\usepackage{float}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{longtable}



\usepackage[utf8]{inputenc}
\begin{document}


\title{Prediction of student alcohol consumption with random forests}

\author{Mikołaj Ciesielski, Michał Sypetkowski}
\setlength\columnsep{0.375in}  \newlength\titlebox \setlength\titlebox{2.25in}
\twocolumn
\maketitle

% \tableofcontents
% \newpage

% \begin{multicols}{2}

\section{Data}
\label{data}

Research is done with dataset: \url{https://www.kaggle.com/uciml/student-alcohol-consumption/}.

The data were obtained in a survey of students
math and portuguese language courses in secondary school.
It contains a lot of interesting social,
gender and study information about students.

The dataset contains in total 395 math-course-students samples and 
649 portuguese-language-students samples. Each sample has 33 attributes.

There are several (382) students that belong to both datasets.
These students can be identified by searching for identical attributes
that characterize each student.

For experiments, we merged both tables.
We replaced students grades attributes
(\texttt{G1}, \texttt{G2} and \texttt{G3})
with
\texttt{M\_G1},
\texttt{M\_G2},
\texttt{M\_G3},
\texttt{P\_G1},
\texttt{P\_G2} and
\texttt{P\_G3}.
    Attribute \texttt{paid} (extra paid classes within the course subject) also concerns particular course, so we created attributes
\texttt{M\_paid} and \texttt{P\_paid}.
Since randomForest package (see section \ref{forest}) doesn't support missing variables handling (e.g. surrogate splits),
they are replaced with mean value in case of grades and by mode value in case of paid attribute.
Dalc and Walc attributes are clustered into one binary attribute -- Drink (see section \ref{clust} for details).
Final dataset consists of 662 examples (36 attributes including class).

\subsection{Clustering Dalc and Walc attributes into one binary attribute}
\label{clust}

We aim to build a model that would perform binary classification --
whether student can be considered drinking alcohol regularly or not.
The dataset provides 2 attributes:
\begin{itemize}
    \item Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)
    \item Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)
\end{itemize}
2D histogram visualization is shown on figure \ref{fig:hist2D}.

First, we standardize Dalc and Walc attributes.
Using k-means algorithm (k=2) with initial centers in (-1,-1) and (1,1), we got clustering as shown in figure \ref{fig:clust}.
In the end, we classify 45.3\% of the students as drinking with proposed clustering.
The results of this clustering may be considered proper in the context of common sense.


% TODO: think about following
% We maximize consistency within 2 clusters of data using Silhouette coeficient
% (\url{https://en.wikipedia.org/wiki/Silhouette_(clustering)}).
% We use L2 distance.

\begin{figure}[!hbt]
    \centering
    \includegraphics[page=1,width=0.5\textwidth]{../Rplots.pdf}
    \caption[]{2D histogram of Dalc and Walc attributes
    \label{fig:hist2D}
    }
\end{figure}

\begin{figure}[!hbt]
    \centering
    \includegraphics[page=2,width=0.5\textwidth]{../Rplots.pdf}
    \caption[]{Clustering with k-means algorithm. (1: non-drinking, 2: drinking).
                Attributes Dalc and Walc are standardized.
    \label{fig:clust}
    }
\end{figure}

\section{Measuring single attributes entropy}
\label{xent}
First, we check how the class is dependent on each single nominal attribute.

Figure \ref{fig:nominalIG} shows information gain values
of single splits by each nominal attribute (in the context of class Drink described in section \ref{clust}).
As we can see, sex of the students gives significantly bigger information gain
than the other nominal attributes, hence it may be very important attribute
in classification.

% Table \ref{table:nominalIG} shows information gain values for all nominal attributes.
% \input{../nominalIG}

\begin{figure}[!hbt]
    \centering
    \includegraphics[page=5,width=0.5\textwidth]{../Rplots.pdf}
    \caption[]{Information gain values for all nominal attributes
    \label{fig:nominalIG}
    }
\end{figure}

\begin{figure}[!hbt]
    \centering
    \includegraphics[page=6,width=0.5\textwidth]{../Rplots.pdf}
    \caption[]{P-values for all numeric attributes
    \label{fig:pval}
    }
\end{figure}


Figure \ref{fig:pval} shows p-values of t-test significance for numeric attributes.
From this plot, we can decide which attributes are statistically important
in the context of our classification problem.
We can see that portuguese language grades show larger importance than math grades.
This is expected because we have around 2 times more samples from portuguese language
class, and we fill missing values with mean value for each attribute (see section \ref{data}).



\section{Experiments with single decision trees}
\label{expSingle}

Accuracy of a single decision tree in general shouldn't be better than in case of random forest,
so we trained several single decision trees to establish
an upper limit of desired error rate for random forests.

We used rpart\footnote{\url{https://cran.r-project.org/web/packages/rpart/rpart.pdf}} package for the experiments.
We trained many models (with different parameters).
We experimented with:
\begin{enumerate}
    \item cp - complexity parameter.
        Any split that does not decrease the overall lack of fit by a factor of cp is not attempted
        (the lower cp is, the more complex the model is)
    \item minbucket - minimum number of observations in any terminal node
    \item maxdepth - maximum tree depth
    \item split - method for calculating how good the split is(Gini index or entropy)
\end{enumerate}
In accuracy measuring, we use cross-validation with 8 partitions
and 10 repetitions (random shuffles of dataset before partitioning).
% Figure \ref{fig:single1} shows best tree for math studens,
% figure \ref{fig:single2} for portuguese-language studens.
% 
\subsection{Results}
\label{singleConc}

Results for all trained decision trees are shown in table \ref{table:singleResults}.
Standard deviation, min, and max error values don't reveal any models
particularly stable or standing out in any other way.
Best mean error is around 33\%.
We observed that in case of decision trees with lowest mean error, 2 attributes are most important:
\begin{itemize}
    \item goout - going out with friends (numeric: from 1 - very low to 5 - very high).
        Splits with this attribute are done by comparing value of this attribute to 3.5
        (less or equal 3 or higher than 3).
    \item sex - student's sex (binary: 'F' - female or 'M' - male).
        Trees use generalization, such that female students are less likely to drink alcohol.
\end{itemize}
Additionally, students grades attributes and famrel attribute (quality of family relationships)
shown some usefulness.
Tree generated on the whole dataset with parameters producing lowest mean error is shown in figure \ref{fig:single}.

\begin{figure}[!hbt]
    \centering
    % \includegraphics[trim={2cm 2cm 2cm 2cm},clip,page=3,width=0.5\textwidth]{../Rplots.pdf}
    \includegraphics[trim={1cm 1cm 1cm 1cm},clip,page=3,width=0.5\textwidth]{../Rplots.pdf}
    \caption[]{Single decision tree built with parameters that give lowest mean error
    \label{fig:single}
    }
\end{figure}

% \begin{figure}[]
%     \caption[]{Best single decision tree for math students that we achieved}
%     \centering
%     \includegraphics[page=4,width=1.0\textwidth]{../Rplots.pdf}
%     \label{fig:single1}
% \end{figure}
% 
% \begin{figure}[]
%     \caption[]{Best single decision tree for portuguese students that we achieved}
%     \centering
%     \includegraphics[page=6,width=1.0\textwidth]{../Rplots.pdf}
%     \label{fig:single2}
% \end{figure}
% 
% 
% \newpage
\section{Experiments with random forests}
\label{forest}
We used randomForest\footnote{\url{https://cran.r-project.org/web/packages/randomForest/randomForest.pdf}} package for the experiments.
We trained many models (with different parameters).
In accuracy measuring, we use cross-validation with 8 partitions
and 10 repetitions (similarly as in \ref{expSingle}).
We experimented with:
\begin{enumerate}
    \item ntree - number of trees in forest
    \item nodesize - minimum size (number of associated examples) of terminal nodes.
    % \item seed - seed used when building a tree
    %         In case of small random forests, it may be important to experiment with different random seed.
    %         Some implementations may use separate random generator objects for each tree, so that
    %         even with different data (we use cross validation), structure will be similar.
    %         Just in case, we decided to try different seeds set in moment of calling \texttt{randomForest}
    %         function from randomForest package.
    \item mtry - number of variables (attributes) randomly sampled as candidates at each split.
    \item maxnodes - maximum number of terminal nodes that trees in the forest can have.
\end{enumerate}

\subsection{Results}
\label{singleConc}
Results for all tested random forests are shown in table \ref{table:forestResults}.
Standard deviation, min, and max error values don't reveal any random forests, which are
particularly stable or stand out in any other way.
Best mean error rate is around 30\%.

% Random forests doesn't produce better results than single decision trees.
% Moreover, mtry parameter equals total attributes count.
% In results, such random forests have similar properties to single decision trees.
% We found out, that the trees in such forests use mostly splits as
% in case of best single decision trees (see section \ref{singleConc})
Variable importance plot for a random forest built on the whole dataset
with parameters that give lowest mean error,
is shown in figure \ref{fig:importance}.

Confusion matrix for such parameters is shown in table \ref{table:convMx}.
We can see that in practice, our model can filter out non drinking students
quite efficiently.
However, in the case when a student is drinking by ground truth, we cannot
estimate the class efficiently (we have almost equal probabilities).

\begin{table}[!hbt]
    \caption{Confusion matrix (ratio) for random forest built with parameters giving lowest mean error
    \label{table:convMx}
    }
\begin{center}
    \begin{tabular}{| l | l | l |}
    \hline
        & non drinking & drinking \\
    \hline
        predicted non drinking  & 0.47 & 0.22 \\
        predicted drinking  &  0.08 & 0.23 \\
    \hline
    \end{tabular}
\end{center}
\end{table}


\begin{figure}[!hbt]
    \centering
    \includegraphics[trim={0 0 0 2cm},clip,page=4,width=0.5\textwidth]{../Rplots.pdf}
    \caption[]{Variable (attribute) importance plot for random forest built with parameters giving lowest mean error
    \label{fig:importance}
    }
\end{figure}

\subsection{Detailed mtry parameter experiment}
\label{mtryExp}

We experimented with changing maxnodes parameter using other parameters values
that achieved best results (ntree=500, nodesize=10 and maxnodes=30).
Mean error plot is shown in figure \ref{fig:detailedMtry}.

\begin{figure}[!hbt]
    \centering
    % \includegraphics[trim={2cm 2cm 2cm 2cm},clip,page=3,width=0.5\textwidth]{../Rplots.pdf}
    \includegraphics[trim={0cm 0cm 0cm 0cm},clip,page=9,width=0.5\textwidth]{../Rplots.pdf}
    \caption[]{Mean error with standard deviations for various mtry parameter values
    \label{fig:detailedMtry}
    }
\end{figure}

As we can see, best results (error rate around 30\%) are achieved by values 4, 6, and 12.
In our case, 6 is rounded square of attributes count, and that strategy usually
gives best results in classification random forest
(e.g. randomForest package use it for calculating default mtry value for classification).

\subsection{Detailed maxnodes parameter experiment}

Similarly to mtry parameter experiment (section \ref{mtryExp}), we test maxnodes parameter.
Mean error plot is shown in figure \ref{fig:detailedMaxnodes}.

\begin{figure}[!hbt]
    \centering
    % \includegraphics[trim={2cm 2cm 2cm 2cm},clip,page=3,width=0.5\textwidth]{../Rplots.pdf}
    \includegraphics[trim={0cm 0cm 0cm 0cm},clip,page=12,width=0.5\textwidth]{../Rplots.pdf}
    \caption[]{Mean error with standard deviations for various maxnodes parameter values
    \label{fig:detailedMaxnodes}
    }
\end{figure}

As we can see, best results (error rate around 30\%) are achieved by values 15 and 30.
It is hard in this case to explain why these values works better than the others.

\subsection{Detailed nodesize parameter experiment}

Similarly, we test nodesize parameter.
Mean error plot is shown in figure \ref{fig:detailedNodesize}.

\begin{figure}[!hbt]
    \centering
    % \includegraphics[trim={2cm 2cm 2cm 2cm},clip,page=3,width=0.5\textwidth]{../Rplots.pdf}
    \includegraphics[trim={0cm 0cm 0cm 0cm},clip,page=15,width=0.5\textwidth]{../Rplots.pdf}
    \caption[]{Mean error with standard deviations for various nodesize parameter values
    \label{fig:detailedNodesize}
    }
\end{figure}

As we can see, best results (error rate around 30\%) are achieved by value 10.
It is hard in this case to explain why this value works better than the others.


% \begin{figure}[]
%     \caption[]{Variable (attribute) importance plot for math students}
%     \centering
%     \includegraphics[page=5,width=1.0\textwidth]{../Rplots.pdf}
%     \label{fig:imp1}
% \end{figure}
% 
% \begin{figure}[]
%     \caption[]{Variable (attribute) importance plot for portuguese language students}
%     \centering
%     \includegraphics[page=7,width=1.0\textwidth]{../Rplots.pdf}
%     \label{fig:imp2}
% \end{figure}
% 

\section{Feature selection}
\label{featSel}
We performed experiment where we exclude attributes that
seem not significant in our classification task.
We empirically select the attributes which seem to have some importance in the context of
values shown in figures \ref{fig:nominalIG}, \ref{fig:pval}, \ref{fig:importance},
and attributes used by our best single decision tree (see figure \ref{fig:single}).

We select nominal attributes:
sex,
Fjob,
higher,
famsize,
reason; and numeric:
goout,
P\_G1,
P\_G2,
P\_G3,
M\_G1,
M\_G2,
M\_G3,
studytime,
absences,
freetime,
famrel,
health,
age,
M\_G1,
nursery,
M\_paid,
Mjob.
We repeated the experiments with single trees and random forests.

\subsection{Results}

Results of best classifiers of each type (single tree or random forest) remains similar 
(mean error around 33\% for single trees, and 30\% for random forests).
Since the results are similar to the results without feature selection,
we don't include tables with detailed results.
Moreover, we can say that single trees and random forests handle
the unimportant attributes properly (unimportant as defined at the beginning of section \ref{featSel}).



\section{Conclusion}
Random forests achieve results only slightly better (mean error rate around 30\%)
than single decision trees (33\%).

Since random forest is widely used algorithm with many successes, we suspect
that it is hard or impossible to find significantly better
student alcohol consumption classification rule
with our dataset.
High-precision classification in this case, may require more detailed
student profile or larger dataset.
Furthermore, the data were obtained in a survey.
That leads to a suspicion, that there may be a lot of false information in
what the high-school students have written in a probably anonymous survey.

% Random forests doesn't perform well on this dataset (no better than single decision trees).
% Most attributes are noisy in terms of single splits (cross entropy -- see section \ref{xent}).
% Best single decision tree turned out to have only 2 splits (see figure \ref{fig:single}).

% TODO
% Decision trees (and random forests) are trying to use these noisy variables
% by greedily splitting them by some information-gain-like measurement.
% hence their accuracy is similar or lower than accuracy of
% a simple decision tree.


% \end{multicols}

\onecolumn
\newpage
\appendix
\section{All single tree classifiers results}
% \label{singleResults}
% \input{singleResults}
\input{../singleResults}

\newpage
\section{All random forest classifiers results}
% \label{forestResults}
% \input{forestResults}
\input{../forestResults}




\end{document}
